name: Task Submission

on:
  pull_request:
    types: [opened, synchronize, reopened]

env:
  PYTHON_VERSION: "3.11"

jobs:
  # ============================================
  # Job 1: Detect task and validate
  # ============================================
  setup:
    name: Setup & Validate
    runs-on: ubuntu-latest
    outputs:
      task_id: ${{ steps.detect.outputs.task_id }}
      week: ${{ steps.detect.outputs.week }}
      has_notebook: ${{ steps.check-files.outputs.has_notebook }}
      has_screenshot: ${{ steps.check-files.outputs.has_screenshot }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect task from branch name
        id: detect
        run: |
          BRANCH="${{ github.head_ref }}"
          echo "Branch: $BRANCH"

          # Flexible task ID extraction - matches multiple patterns:
          # - task-1.2, task-1.2-description
          # - feature/1.2-something
          # - 1.2-fix-bug
          # - fix/task-1.2
          TASK_ID=$(echo "$BRANCH" | grep -oE '[0-9]+\.[0-9]+' | head -1 || echo "")

          # Get week number
          WEEK=$(echo "$TASK_ID" | cut -d'.' -f1)

          echo "task_id=$TASK_ID" >> $GITHUB_OUTPUT
          echo "week=$WEEK" >> $GITHUB_OUTPUT

          echo "Detected: Task=$TASK_ID, Week=$WEEK"

      - name: Validate branch naming
        run: |
          BRANCH="${{ github.head_ref }}"
          # Flexible validation - just needs to contain a task number pattern (X.Y)
          if [[ ! "$BRANCH" =~ [0-9]+\.[0-9]+ ]]; then
            echo "::error::Branch name must contain a task number (e.g., 1.2)"
            echo "Valid examples: task-1.2, task-1.2-fix-bug, feature/1.2, 1.2-my-work"
            exit 1
          fi

      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v41

      - name: Check for special files
        id: check-files
        run: |
          HAS_NOTEBOOK=false
          HAS_SCREENSHOT=false

          for file in ${{ steps.changed-files.outputs.all_changed_files }}; do
            if [[ "$file" == *.ipynb ]]; then
              HAS_NOTEBOOK=true
            fi
            if [[ "$file" == *.png ]] || [[ "$file" == *.jpg ]] || [[ "$file" == *.pdf ]]; then
              HAS_SCREENSHOT=true
            fi
          done

          echo "has_notebook=$HAS_NOTEBOOK" >> $GITHUB_OUTPUT
          echo "has_screenshot=$HAS_SCREENSHOT" >> $GITHUB_OUTPUT

  # ============================================
  # Job 2: Linting
  # ============================================
  lint:
    name: Lint Code
    runs-on: ubuntu-latest
    needs: setup

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install linters
        run: pip install ruff black

      - name: Run ruff
        id: ruff
        continue-on-error: true
        run: |
          ruff check . --exclude tests/ --output-format=github || echo "ruff_failed=true" >> $GITHUB_OUTPUT

      - name: Check formatting with black
        id: black
        continue-on-error: true
        run: |
          black --check . --extend-exclude tests/ || echo "black_failed=true" >> $GITHUB_OUTPUT

      - name: Lint summary
        run: |
          if [[ "${{ steps.ruff.outputs.ruff_failed }}" == "true" ]] || [[ "${{ steps.black.outputs.black_failed }}" == "true" ]]; then
            echo "::warning::Linting issues found. Consider fixing them."
            echo ""
            echo "To fix formatting: black ."
            echo "To check issues: ruff check . --fix"
          else
            echo "Linting passed"
          fi

  # ============================================
  # Job 3: Run Tests
  # ============================================
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    needs: setup
    outputs:
      passed: ${{ steps.test.outputs.passed }}
      test_score: ${{ steps.test.outputs.test_score }}
      output: ${{ steps.test.outputs.output }}
      failed_tests: ${{ steps.failures.outputs.failed_tests }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install pytest pytest-json-report pandas pandera duckdb nbclient nbformat PyPDF2

      - name: Execute notebooks
        if: needs.setup.outputs.has_notebook == 'true'
        run: |
          for nb in $(find submissions/ -name "*.ipynb" 2>/dev/null); do
            echo "Executing: $nb"
            python -m nbclient "$nb" --timeout 120 || {
              echo "::error::Notebook execution failed: $nb"
              exit 1
            }
          done

      - name: Run task tests
        id: test
        run: |
          TASK_ID="${{ needs.setup.outputs.task_id }}"

          # Convert task ID to test file name (2.1 -> test_2_1.py)
          TEST_FILE="tests/test_${TASK_ID//./_}.py"

          echo "Looking for test file: $TEST_FILE"

          if [ ! -f "$TEST_FILE" ]; then
            echo "No test file found for task $TASK_ID"
            echo "passed=skip" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Run tests with JSON report, capture exit code
          set +e
          pytest "$TEST_FILE" -v \
            --json-report --json-report-file=test-report.json 2>&1 | tee test-output.txt
          TEST_EXIT_CODE=${PIPESTATUS[0]}
          set -e

          # Parse results
          if [ -f test-report.json ]; then
            PASSED=$(python -c "import json; r=json.load(open('test-report.json')); print(r['summary'].get('passed', 0))")
            FAILED=$(python -c "import json; r=json.load(open('test-report.json')); print(r['summary'].get('failed', 0))")
            TOTAL=$(python -c "import json; r=json.load(open('test-report.json')); print(r['summary'].get('total', 0))")

            echo "Results: $PASSED passed, $FAILED failed, $TOTAL total"

            # Compute test_score as percentage of non-skipped tests
            # Skips are expected (e.g., task 1.3 accepts either code fix or README)
            TEST_SCORE=$(python -c "import json; r=json.load(open('test-report.json')); s=r['summary']; run=s.get('passed',0)+s.get('failed',0); print(int(100*s['passed']/run) if run>0 else 0)")
            echo "test_score=$TEST_SCORE" >> $GITHUB_OUTPUT

            if [ "$FAILED" -eq "0" ] && [ "$PASSED" -gt "0" ]; then
              echo "passed=true" >> $GITHUB_OUTPUT
              echo "✅ All tests passed! Score: $TEST_SCORE%"
            else
              echo "passed=false" >> $GITHUB_OUTPUT
              echo "❌ Some tests failed. Please fix them before merging."
            fi
          else
            echo "No test report generated"
            echo "passed=false" >> $GITHUB_OUTPUT
            echo "test_score=0" >> $GITHUB_OUTPUT
          fi

          # Exit with the test result so the job properly fails
          exit $TEST_EXIT_CODE

      - name: Extract failed test details
        id: failures
        if: always()
        run: |
          if [ -f test-report.json ]; then
            FAILED_TESTS=$(python -c "
          import json
          r = json.load(open('test-report.json'))
          failures = []
          for t in r.get('tests', []):
              if t.get('outcome') == 'failed':
                  name = t.get('nodeid', '').split('::')[-1]
                  msg = (t.get('call') or {}).get('crash', {}).get('message', '')[:200]
                  failures.append({'name': name, 'error': msg})
          print(json.dumps(failures[:10]))
          " 2>/dev/null || echo "[]")
            echo "failed_tests=$FAILED_TESTS" >> $GITHUB_OUTPUT
          else
            echo "failed_tests=[]" >> $GITHUB_OUTPUT
          fi

      - name: Upload test report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-report
          path: |
            test-report.json
            test-output.txt
          if-no-files-found: ignore

  # ============================================
  # Job 5: Notify Backend
  # ============================================
  notify:
    name: Notify Backend
    runs-on: ubuntu-latest
    needs: [setup, lint, test]
    if: always()

    steps:
      - name: Send results to backend
        env:
          BACKEND_URL: ${{ secrets.BACKEND_URL }}
          WEBHOOK_SECRET: ${{ secrets.WEBHOOK_SECRET }}
          FAILED_TESTS: ${{ needs.test.outputs.failed_tests }}
          RAW_TEST_PASSED: ${{ needs.test.outputs.passed }}
          RAW_TEST_SCORE: ${{ needs.test.outputs.test_score }}
          RAW_LINT_PASSED: ${{ needs.lint.result == 'success' }}
        run: |
          if [ -z "$BACKEND_URL" ]; then
            echo "BACKEND_URL not configured, skipping notification"
            exit 0
          fi

          # Determine overall status
          LINT_PASSED="$RAW_LINT_PASSED"
          TEST_PASSED="$RAW_TEST_PASSED"
          TEST_SCORE="$RAW_TEST_SCORE"

          # Convert skip to false — skipped tests should not count as passing
          if [ "$TEST_PASSED" = "skip" ]; then
            TEST_PASSED="false"
            TEST_SCORE="0"
          fi

          if [ -z "$FAILED_TESTS" ]; then
            FAILED_TESTS="[]"
          fi

          # Use jq to safely construct JSON (handles quotes/newlines in failed_tests)
          PAYLOAD=$(jq -n \
            --arg event "pr_check_complete" \
            --argjson pr_number "${{ github.event.pull_request.number }}" \
            --arg pr_url "${{ github.event.pull_request.html_url }}" \
            --arg task_id "${{ needs.setup.outputs.task_id }}" \
            --arg student_github "${{ github.event.pull_request.user.login }}" \
            --arg branch_name "${{ github.head_ref }}" \
            --argjson lint_passed "$LINT_PASSED" \
            --arg test_passed "$TEST_PASSED" \
            --argjson test_score "${TEST_SCORE:-0}" \
            --arg repo "${{ github.repository }}" \
            --arg track "ds" \
            --argjson failed_tests "$FAILED_TESTS" \
            '{
              event: $event,
              pr_number: $pr_number,
              pr_url: $pr_url,
              task_id: $task_id,
              student_github: $student_github,
              branch_name: $branch_name,
              lint_passed: $lint_passed,
              test_passed: $test_passed,
              test_score: $test_score,
              repo: $repo,
              track: $track,
              failed_tests: $failed_tests
            }')

          curl -X POST "${BACKEND_URL}/webhooks/github" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${WEBHOOK_SECRET}" \
            -d "$PAYLOAD" || echo "Backend notification failed (non-fatal)"

          echo "Backend notified"

